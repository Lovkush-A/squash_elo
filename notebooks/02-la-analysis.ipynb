{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64117de",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, I create and run functions to:\n",
    "* create the ELO ratings\n",
    "* measure the performance of ELO ratings\n",
    "\n",
    "Using default value for the hyper-parameter `K` of 32, we see that the predictions from ELO are under-confident: if the ELO rating predicts that a player will win 80%, they actually win about 90% of the time. Similarly for other percentages. After doing a (manual) gridsearch, we see that `K=100` gives predictions that are calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a68dd",
   "metadata": {},
   "source": [
    "### Setup and data loading\n",
    "\n",
    "The data that is loaded is the outcome of running the notebook `01-la-processing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ce745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_processed = \"../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fe97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv(dir_processed+\"matches.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c72bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860a5f1",
   "metadata": {},
   "source": [
    "### ELO analysis\n",
    "\n",
    "Define and run the various functions to carry out and evaluate the elo rating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ce4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_predicted_score(\n",
    "    rating1: float,\n",
    "    rating2: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the predicted score using ELO rating system\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rating1, rating2: float\n",
    "        ELO ratings of two players\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The predicted score of a player with rating1 against\n",
    "        a player with rating2 using the ELO rating system\n",
    "    \"\"\"\n",
    "    return 1 / (1 + 10**((rating2 - rating1) / 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_ratings(\n",
    "    rating_winner: float,\n",
    "    rating_loser: float,\n",
    "    predicted_score: float,\n",
    "    K: float = 32,\n",
    ") -> (float, float):\n",
    "    \"\"\"\n",
    "    Calculate new elo ratings after a single match.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rating_winner, rating_loser: float\n",
    "        ELO ratings of the winner and loser, respectively, before\n",
    "        the match took place.\n",
    "    predicted_score: float in range [0, 1]\n",
    "        The expected score of the winner of the match.\n",
    "    K: float, default 32\n",
    "        Constant that determines how much the ratings are adjusted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_rating_winner, new_rating_loser: float\n",
    "        New ELO ratings\n",
    "    \"\"\"\n",
    "    delta_rating = K * (1 - predicted_score)\n",
    "\n",
    "    new_rating_winner = rating_winner + delta_rating\n",
    "    new_rating_loser = rating_loser - delta_rating\n",
    "\n",
    "    return new_rating_winner, new_rating_loser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually test the above functions\n",
    "\n",
    "for delta in range(-500, 501, 50):\n",
    "    predicted_score = calculate_predicted_score(delta, 0)\n",
    "    new_rating_winner, new_rating_loser = calculate_new_ratings(\n",
    "        delta, 0, predicted_score\n",
    "    )\n",
    "    \n",
    "    print(f'Old winner rating: {delta:3}.')\n",
    "    print(f'Old loser rating: 0')\n",
    "    print(f'Predicted score: {predicted_score}')\n",
    "    print(f'New winner rating: {new_rating_winner}')\n",
    "    print(f'New loser rating: {new_rating_loser}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f90394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ratings_single(player_ratings, winner_name, loser_name, K=32):\n",
    "    \"\"\"\n",
    "    Update ratings based on a single new result.\n",
    "\n",
    "    If winner_name or loser_name is not already in the\n",
    "    player_ratings dictionary, then a fresh entry with a rating\n",
    "    of 1500 is created before updating based on new results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    player_ratings: Dict[str, float]\n",
    "        dictionary of player ratings\n",
    "    winner_name, loser_name: str,\n",
    "        name of winner and loser\n",
    "    K: float, default 32\n",
    "        Constant that determines how much the ratings are adjusted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    player_ratings\n",
    "        updated player ratings\n",
    "    rating_winner_old\n",
    "    rating_winner_new\n",
    "    rating_loser_old\n",
    "    rating_loser_new\n",
    "    predicted_score\n",
    "    \"\"\"\n",
    "    if winner_name not in player_ratings:\n",
    "        player_ratings[winner_name] = 1500\n",
    "    rating_winner_old = player_ratings[winner_name]\n",
    "\n",
    "    if loser_name not in player_ratings:\n",
    "        player_ratings[loser_name] = 1500\n",
    "    rating_loser_old = player_ratings[loser_name]\n",
    "\n",
    "    predicted_score = calculate_predicted_score(rating_winner_old, rating_loser_old)\n",
    "\n",
    "    rating_winner_new, rating_loser_new = calculate_new_ratings(\n",
    "        rating_winner_old, rating_loser_old, predicted_score, K=K\n",
    "    )\n",
    "    \n",
    "    player_ratings[winner_name] = rating_winner_new\n",
    "    player_ratings[loser_name] = rating_loser_new\n",
    "\n",
    "    return (\n",
    "        player_ratings,\n",
    "        rating_winner_old,\n",
    "        rating_winner_new,\n",
    "        rating_loser_old,\n",
    "        rating_loser_new,\n",
    "        predicted_score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33befe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually test update_ratings\n",
    "\n",
    "test_ratings = {'a': 1000, 'b': 900}\n",
    "\n",
    "test_ratings, _, _, _, _, _ = update_ratings_single(test_ratings, 'a', 'b')\n",
    "test_ratings, _, _, _, _, _ = update_ratings_single(test_ratings, 'c', 'd')\n",
    "test_ratings, _, _, _, _, _ = update_ratings_single(test_ratings, 'd', 'c')\n",
    "\n",
    "test_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06bbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ratings(matches: pd.DataFrame, K=32, player_ratings={}) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Update elo ratings based on all match results in `matches`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matches: pd.DataFrame\n",
    "        dataframe of match history\n",
    "    K: float, default 32\n",
    "        Constant that determines how much the ratings are adjusted\n",
    "    player_ratings: Dict[str, float]\n",
    "        dictionary of players' elo ratings before the matches\n",
    "        in `matches` were played\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    player_ratings: Dict[str, float]\n",
    "        updated player_ratings\n",
    "    pd.DataFrame\n",
    "        copy of matches dataframe with new columns for:\n",
    "        * rating_winner_old\n",
    "        * rating_winner_new\n",
    "        * rating_loser_old\n",
    "        * rating_loser_new\n",
    "        * predicted_score\n",
    "    \"\"\"\n",
    "    ratings_winner_old = []\n",
    "    ratings_winner_new = []\n",
    "    ratings_loser_old = []\n",
    "    ratings_loser_new = []\n",
    "    predicted_scores = []\n",
    "\n",
    "    df = matches.copy()\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        winner_name = row.winner_name\n",
    "        loser_name = row.loser_name\n",
    "\n",
    "        (\n",
    "            player_ratings,\n",
    "            rating_winner_old,\n",
    "            rating_winner_new,\n",
    "            rating_loser_old,\n",
    "            rating_loser_new,\n",
    "            predicted_score,\n",
    "        ) = update_ratings_single(player_ratings, winner_name, loser_name, K=K)\n",
    "        \n",
    "        ratings_winner_old.append(rating_winner_old)\n",
    "        ratings_winner_new.append(rating_winner_new)\n",
    "        ratings_loser_old.append(rating_loser_old)\n",
    "        ratings_loser_new.append(rating_loser_new)\n",
    "        predicted_scores.append(predicted_score)\n",
    "    \n",
    "    df['rating_winner_old'] = ratings_winner_old\n",
    "    df['rating_winner_new'] = ratings_winner_new\n",
    "    df['rating_loser_old'] = ratings_loser_old\n",
    "    df['rating_loser_new'] = ratings_loser_new\n",
    "    df['predicted_score'] = predicted_scores\n",
    "    \n",
    "    return player_ratings, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b05073",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ratings, matches = update_ratings(matches)\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_player_history(df: pd.DataFrame, player: str) -> None:\n",
    "    \"\"\"\n",
    "    View all results of a player.\n",
    "    \n",
    "    Prints the following for all games that `player` played:\n",
    "    * name of winner\n",
    "    * name of loser\n",
    "    * winner's elo rating (before the match)\n",
    "    * loser's elo rating (before the match)\n",
    "    * winner's seed in the tournament\n",
    "    * loser's seed in the tournament\n",
    "    * predicted score from elo ratings for the match\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        dataframe of matches as outputted by `update_ratings`\n",
    "    player: str\n",
    "        name of the player\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    indices = (df.winner_name == player) | (df.loser_name == player)\n",
    "    df_player = df[indices]\n",
    "    \n",
    "    for _,row in df_player.iterrows():\n",
    "        w = row.winner_name\n",
    "        wr = row.rating_winner_old\n",
    "        ws = row.winner_seed\n",
    "        l = row.loser_name\n",
    "        lr = row.rating_loser_old\n",
    "        ls = row.loser_seed\n",
    "        pred = row.predicted_score\n",
    "        print(f'{w[0:10]:10} beat {l[0:10]:10} {wr:.0f} vs {lr:.0f}   {ws:3} vs {ls:3}   {pred:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_player_history(matches, 'Ramy Ashour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_calibration(df_input: pd.DataFrame, N: int = 2) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Evaluate how well calibrated the ELO ratings are.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input\n",
    "        Dataframe as outputted by calculate_elo\n",
    "    N: int\n",
    "        Number of times each bucket of size 0.1 is broken up.\n",
    "        See the index of the returned pd.Series for an example\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        * index is predicted score, rounded to nearest 0.1/N. For\n",
    "          example, if N=2, then rounded to nearest 0.05, so index is\n",
    "          0.5, 0.55, 0.6,...,0.95, 1\n",
    "        * values are the average true score of matches whose predicted\n",
    "        score is in that bucket\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "\n",
    "    df[\"predicted_score_better_player\"] = df.predicted_score.apply(\n",
    "        lambda x: round(N * x, 1) / N if x > 0.5 else 1 - round(N * x, 1) / N\n",
    "    )\n",
    "\n",
    "    df[\"true_score_better_player\"] = df.predicted_score.apply(\n",
    "        lambda x: 1 if x > 0.5 else 0\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        df\n",
    "        .groupby(\"predicted_score_better_player\")\n",
    "        .agg({\"true_score_better_player\": [\"count\", \"mean\"]})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_calibration(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e767307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_calibration_seeds(df: pd.DataFrame, seeds, N: int = 2, null=False):\n",
    "    \"\"\"\n",
    "    Evaluate calibration of ratings for players' whose seed is in `seeds`\n",
    "    \"\"\"\n",
    "    indices = df.winner_seed.isin(seeds) & df.loser_seed.isin(seeds)\n",
    "    \n",
    "    if null:\n",
    "        null_loss = df.winner_seed.isin(seeds) & df.loser_seed.isnull()\n",
    "        null_win = df.winner_seed.isnull() & df.loser_seed.isin(seeds)\n",
    "        indices = indices | null_loss | null_win\n",
    "    \n",
    "    return evaluate_calibration(df[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_calibration_seeds(matches, ['1','2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a91af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_calibration_seeds(matches, ['1'], null=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40242aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_calibration_seeds(matches, ['2'], null=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc5041",
   "metadata": {},
   "source": [
    "## Tuning for K\n",
    "Looking at the above, we see that the predicted_scores are underconfident: the predicted scores are generally smaller than the observed scores. This suggests that the ELO rating is not updating enough after each game, i.e. that K is too small. Hence, I will try various values of K and see which gives better calibrated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c000db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_full_analysis(file=dir_processed+\"matches.csv\", K=32):\n",
    "    matches = pd.read_csv(dir_processed+\"matches.csv\", index_col=0)\n",
    "    _, matches = update_ratings(matches, K=K)\n",
    "    return evaluate_calibration(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "do_full_analysis(K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e806d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=50\n",
    "do_full_analysis(K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=100\n",
    "do_full_analysis(K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=200\n",
    "do_full_analysis(K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049da4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=500\n",
    "do_full_analysis(K=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32596dc",
   "metadata": {},
   "source": [
    "Based on the above, it looks like K=100 is a good value for K."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
